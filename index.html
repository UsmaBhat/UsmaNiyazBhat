<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Google tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-KEFJZWLLB5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-KEFJZWLLB5');
  </script>

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
  
  <script type="text/javascript" src="js/hidebib.js"></script>

  <title>Usma Niyaz</title>

  <meta name="Usma Niyaz's Homepage" http-equiv="Content-Type" content="Usma Niyaz's Homepage">
  <meta name="description" content="Usma's personal website with latest updates about his research.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONTS -->
  <!-- <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
   -->
  <!-- Ubuntu Font -->
  <!-- <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@300;400;500;700&display=swap" rel="stylesheet"> -->

  <!-- ROBOTO Font -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">




  <!-- JavaScript to scramble emails -->
  <!-- email scramble removed: using plain mailto links instead -->

  <!-- Usma's Custom CSS -->
  <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
  <link rel="icon" type="image/png" href="images/title-icon.png">

</head>

<body>
  <div class="topbar">
    <ul class="nav fixed-top justify-content-center bg-dark">
      <li class="nav-item">
        <a class="nav-link" href="#introduction">Introduction</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#news">News</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#publications">Publications</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#ta">TA</a>
      </li>
      <li class="nav-item" >
        <a class="nav-link" href="resources.html">Resources</a>
      </li>
      <!-- <li class="nav-item" style="text-align: center;">
        <img src="images/halo-legendary-icon.png" width="32" height="32">
      </li> -->
    </ul>
  </div>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <!-- Introduction -->
          <!-- Introduction -->
          <!-- Introduction -->
          <!-- Introduction -->
          <!-- Introduction -->
          <!-- Introduction -->
          <table id="introduction">
            <tbody>
              <tr>
                <td style="padding-left:3.5%;width: 63%;vertical-align:middle;">
                  <p class="text-center">
                    <firstname>Usma </firstname><name>Niyaz</name>
                    <br>
                    <br>
                    <b>email (academics): </b>
                    <a href="mailto:usma.20csz0015@iitrpr.ac.in">usma.20csz0015@iitrpr.ac.in</a>
                    <br>
                    <b>email (personal): </b>
                    <a href="mailto:usmabhatt@gmail.com">usmabhatt@gmail.com</a>
                  </p>
                  
                  <p class="text-justify">I am a PhD candidate in the Department of Computer Science and Engineering at the <a href="http://iitrpr.ac.in/">Indian Institute of Technology Ropar</a>, India, under the supervision of <a href="https://drbathula.github.io/">Dr Deepti R Bathula</a>.
                  </p>

                  <p class="text-justify">My research interests focus on model compression strategies and knowledge distillation to develop efficient and lightweight deep learning models for deployment in resource-constrained clinical settings.</p>
                  <p class="text-center">
                    <a href="https://drive.google.com/file/d/1iaSajn3FY8-lt9HFrq04ZMsKVN4FLUD6/view?usp=sharing" class="boxed">CV</a>
                    <a href="https://github.com/UsmaBhat" class="boxed">GitHub</a>
                    <a href="https://scholar.google.com/citations?user=n015vG4AAAAJ&hl=en" class="boxed">Google
                      Scholar</a>
                    <a href="https://www.linkedin.com/in/usma-bhat-2ab709134/" class="boxed">LinkedIn</a>
                  </p>
                </td>
                <td style="padding-top:20px;width:37%;max-width:100%;text-align: center;">
                <img class="profileimage" alt="profile photo" src="images/usma_crop.png" width="120" height="250"
                    style="border-radius:15px;border:1px solid black;">
                </td>
              </tr>
            </tbody>
          </table>

          <!-- News -->
          <!-- News -->
          <!-- News -->
          <!-- News -->
          <!-- News -->
          <!-- News -->
          <table id="news">
            <tbody>
              <tr>
                <td style="padding-top:10px;padding-left:20px;padding-bottom: 20px;width:100%;vertical-align:middle">
                  <hr class="section-divider">
                  <heading>Recent News</heading>
                </td>
              </tr>
              <tr>
                <td>
                  <ul class=news>
                    <li><span class="badge bg-secondary">Jan 2026</span> <em>Prototypical Aggregate Network — "Boosting Few-Shot Learning for Medical Image Classification"</em> published in <strong> Multimedia Tools and Applications </strong>, 2026.
                      <li><span class="badge bg-secondary">Oct 2025</span> <em>"ShapeDistill: Shape-Constrained Knowledge Distillation for Medical Segmentation"</em> — accepted at the 2025 <strong>18th International Conference on Machine Vision</strong>.
                      <li><span class="badge bg-secondary">Dec 2024</span> <em> Attended <strong> CODS-COMAD </strong>, IIT Jodhpur
                    <li><span class="badge bg-secondary">Jun 2025</span> <em>"MRI-to-PET Cross-Modality Translation using GLA-GAN"</em> published in <strong>The Journal of Precision Medicine: Health and Disease, 2025.</strong>
                    <li><span class="badge bg-secondary">Feb 2025</span> Awarded <strong>Society of Women Engineers (SWE) scholarship</strong> for the 2025–2026 academic year.
                    <li><span class="badge bg-secondary">Jan 2024</span> <em>"Leveraging Different Learning Styles for Improved Knowledge Distillation in Biomedical Imaging"</em> published in <strong>Computers in Biology and Medicine (2024)</strong>
                    <li><span class="badge bg-secondary">May 2024</span> Presented our work "<em>Wavelet-Based Feature Compression for Improved Knowledge Distillation</em>", at 21st IEEE International Symposium on Biomedical Imaging (ISBI), Athens, Greece.
                    <li><span class="badge bg-secondary">Apr 2024</span> Received <strong>ANRF (formerly SERB) travel grant</strong> to present at ISBI 2024.
                    <li><span class="badge bg-secondary">May 2024</span> Multiple conference papers accepted (ISBI 2024, MICCAI 2024 and related workshops) — see Publications section for details.
                    <li><span class="badge bg-secondary">Dec 2023</span> Attended The Indian Symposium on Machine Learning (IndoML), IIT Bombay.
                   
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Publications -->
          <!-- Publications -->
          <!-- Publications -->
          <!-- Publications -->
          <!-- Publications -->
          <!-- Publications -->
          <table id="publications" width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
            <tbody>
              <!-- Heading -->
                        <tr>
                <td style="padding-top:10px;padding-left:20px;vertical-align:middle">
                  <hr class="section-divider">
                  <heading>Publications</heading>
                </td>
              </tr>

              <!-- Publication Entries -->
              <tr>
                <td width="20%" valign="top" align="center"><a href="images/mtab.png" target="_blank" title="Open image (HD)">
                <img src="images/mtab.png" alt="mtab" width="160" height="120" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="#" title="Link coming soon">
                    <papertitle>Prototypical Aggregate Network - Boosting Few-Shot Learning for Medical Image Classification</papertitle>
                  </a>
                  <br>
                  Ranjana Roy Chowdhury, <strong> Usma Niyaz</strong>, Deepti R. Bathula
                  <br>
                  <em>Multimedia Tools and Applications</em>, 2026
              
                  <div class="paperlinks" id="trau1">
                  <a href="javascript:toggleblock('whirl_abs1')" class="boxed boxed_publication">abstract</a>
                  <a href="#" class="boxed boxed_publication">link (coming soon)</a>
                  <!-- <a href="https://www.dropbox.com/s/5adrl9tyx6s67ii/Poster_ISBI_2022.pdf?dl=0" class="boxed boxed_publication">poster</a> -->
                  <a shape="rect" href="javascript:togglebib('trau1')" class="togglebib boxed boxed_publication">bibtex (coming soon)</a>

                  <p align="justify"> <i id="whirl_abs1" style="display: none;">Prototypical Networks have proven effective for metric-based few-shot learning,
                      enabling models to generalize from limited labeled examples. However, their
                      application to medical image analysis remains challenging due to the complex,
                      high-variability nature of medical images and the need for robust, domain-specific
                      feature extraction. This work propose Prototypical Aggregate Network (PANet),
                      an enhanced variant designed specifically for few-shot medical image classifica-
                      tion. PANet addresses two key challenges: (a) it incorporates spectral components
                      using Discrete Wavelet Transform (DWT) to explicitly capture texture and
                      frequency information relevant for pathology localization, and (b) it mitigates
                      information loss by aggregating intermediate feature embeddings via depth-wise
                      averaging, allowing downstream layers to benefit from earlier-layer morpholog-
                      ical cues. PANet outperforms several state-of-the-art few-shot learning models
                      in low-shot settings, achieving average accuracy gains of 2.8% and 1.55% in 2-
                      way 3-shot and 5-shot classification tasks, respectively, on the BreakHis dataset.
                      Furthermore, it demonstrates statistically significant improvements over compa-
                      rable architectures and achieves competitive performance against much deeper
                      and more complex models on the PathMNIST and BloodMNIST datasets. Quali-
                      tative results using explainability methods further validate PANet's capability to
                      localize and distinguish subtle morphological patterns, enhancing interpretability
                      and supporting its potential for real-world clinical deployment.</i></p>
              
                <pre xml:space="preserve" style="display: none;">
                  </pre>
                  </div>
                </td>
              </tr>


            <tr>
                <td width="20%" valign="top" align="center"><a href="images/shapedistill.png" target="_blank" title="Open image (HD)">
                <img src="images/shapedistill.png" alt="shapedistill" width="160" height="120" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="#" title="Link coming soon">
                    <papertitle>ShapeDistill: Shape-Constrained Knowledge Distillation for Medical Segmentation</papertitle>
                  </a>
                  <br>
                  <strong> Usma Niyaz</strong>, Deepti R. Bathula
                  <br>
                  <em>18th International Conference on Machine Vision</em>, 2025
              
                  <div class="paperlinks" id="trau2">
                  <a href="javascript:toggleblock('whirl_abs2')" class="boxed boxed_publication">abstract</a>
                  <a href="#" class="boxed boxed_publication">link (coming soon)</a>
                  <!-- <a href="https://www.dropbox.com/s/5adrl9tyx6s67ii/Poster_ISBI_2022.pdf?dl=0" class="boxed boxed_publication">poster</a> -->
                  <a shape="rect" href="javascript:togglebib('trau2')" class="togglebib boxed boxed_publication">bibtex (coming soon)</a>

                  <p align="justify"> <i id="whirl_abs2" style="display: none;">Deploying high-quality deep learning models for medical image segmentation in remote or resource-constrained
                    environments remains a significant challenge due to limited computational resources and infrastructure. In
                    this work, we introduce a shape-aware model compression framework that enhances segmentation performance
                    while ensuring model efficiency. Central to our approach is a novel FiLM-attention-based Variational Autoen-
                    coder (VAE), trained directly on ground truth masks to learn compact and expressive latent representations
                    of anatomical structures. The decoder incorporates both Feature-wise Linear Modulation (FiLM) and spatial
                    attention mechanisms to enable robust reconstruction and preserve global shape information. We then apply a
                    two-stage knowledge distillation process: first, we transfer structural knowledge from the shape-aware VAE to a
                    U-Net segmentation model (teacher); second, we distill this shape-constrained teacher into lightweight student
                    networks. These student models inherit both semantic precision and anatomical coherence, maintaining high
                    segmentation accuracy and shape consistency. Across four benchmark datasets, our Shape-Constrained (SC)
                    models demonstrated consistent improvements over baseline U-Net architectures. Specifically, they achieved
                    average IoU gains of +4.5%, +4.4%, and +4.0%, and DSC gains of +1.3%, +1.2%, and +1.7% with
                    ResNet34, ResNet18, and MobileNet backbones, respectively.</i></p>
              
                <pre xml:space="preserve" style="display: none;">
                  </pre>
                  </div>
                </td>
              </tr>

              <tr>
                <td width="20%" valign="top" align="center"><a href="images/GLA-GAN-Arch.jpg" target="_blank" title="Open image (HD)">
                <img src="images/GLA-GAN-Arch.jpg" alt="glagan" width="160" height="80" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="https://www.sciencedirect.com/science/article/pii/S3050632825000046" target="_blank">
                    <papertitle>MRI-to-PET Cross-Modality Translation using Globally & Locally Aware GAN (GLA-GAN) for Multi-Modal Diagnosis of Alzheimer's Disease</papertitle>
                  </a>
                  <br>
                 Apoorva Sikka, Skand Peri, Jitender Singh Virk, <strong> Usma Niyaz</strong>, Deepti R. Bathula
                  <em>The Journal of Precision Medicine: Health and Disease</em>, 2025
              
                  <div class="paperlinks" id="trau3">
                  <a href="javascript:toggleblock('whirl_abs3')" class="boxed boxed_publication">abstract</a>
                  <a href="https://www.sciencedirect.com/science/article/pii/S3050632825000046" class="boxed boxed_publication">journal</a>
                  <!-- <a href="https://www.dropbox.com/s/5adrl9tyx6s67ii/Poster_ISBI_2022.pdf?dl=0" class="boxed boxed_publication">poster</a> -->
                  <a shape="rect" href="javascript:togglebib('trau3')" class="togglebib boxed boxed_publication">bibtex</a>

                  <p align="justify"> <i id="whirl_abs3" style="display: none;">Medical imaging datasets are inherently high dimensional, with large variability and low sample sizes that limit the effectiveness of deep learning algorithms. Recently, generative adversarial networks (GANs) with the ability to synthesize realistic images have shown great potential as an alternative to standard data augmentation techniques. Our work focuses on the cross-modality synthesis of fluorodeoxyglucose (FDG) Positron Emission Tomography (PET) scans from structural Magnetic Resonance (MR) images using generative models to facilitate multi-modal diagnosis of Alzheimer’s disease (AD). Specifically, we propose a novel end-to-end, globally and locally aware image-to-image translation GAN (GLA-GAN) with a multi-path architecture that enforces global structural integrity and fidelity to local details. We further supplement the standard adversarial loss with voxel-level intensity, multi-scale structural similarity (MS-SSIM), and region-of-interest (ROI) based loss components that reduce reconstruction error, enforce structural consistency at different scales, and perceive a variation in regional sensitivity to AD, respectively. Experimental results demonstrate that our GLA-GAN not only generates synthesized FDG-PET scans with enhanced image quality but also superior clinical utility in improving AD diagnosis compared to state-of-the-art models. Finally, we attempt to interpret some of the internal units of the GAN that are closely related to this specific cross-modality generation task.</i></p>
              
              <pre xml:space="preserve" style="display: none;">
@article{SIKKA2025100004,
title = {MRI-to-PET cross-modality translation using globally & locally aware GAN (GLA-GAN) for multi-modal diagnosis of Alzheimer’s disease},
journal = {The Journal of Precision Medicine: Health and Disease},
volume = {2},
pages = {100004},
year = {2025},
issn = {3050-6328},
doi = {https://doi.org/10.1016/j.premed.2025.100004},
url = {https://www.sciencedirect.com/science/article/pii/S3050632825000046},
author = {Apoorva Sikka and Skand Peri and Jitender Singh Virk and Usma Niyaz and Deepti R. Bathula},
keywords = {Alzheimer’s classification, Cross-modality estimation, Generative adversarial networks, Medical imaging},
}
    
    
              </pre>
                  </div>
                </td>
              </tr>


            <tr>
                <td width="20%" valign="top" align="center"><a href="images/isci.png" target="_blank" title="Open image (HD)">
                <img src="images/isci.png" alt="isci2025" width="160" height="80" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="https://ieeexplore.ieee.org/abstract/document/11167098" target="_blank">
                    <papertitle>Dual-Level Adaptive Sampling for Enhanced Few-Shot Medical Image Classification</papertitle>
                  </a>
                  <br>
                 Ranjana Roy Chowdhury, <strong> Usma Niyaz</strong>, Deepti R. Bathula
                  <em>2025 IEEE 7th Symposium on Computers & Informatics (ISCI)</em>, 2025
              
                  <div class="paperlinks" id="trau4">
                  <a href="javascript:toggleblock('whirl_abs4')" class="boxed boxed_publication">abstract</a>
                  <a href="https://ieeexplore.ieee.org/abstract/document/11167098" class="boxed boxed_publication">journal</a>
                  <!-- <a href="https://www.dropbox.com/s/5adrl9tyx6s67ii/Poster_ISBI_2022.pdf?dl=0" class="boxed boxed_publication">poster</a> -->
                  <a shape="rect" href="javascript:togglebib('trau4')" class="togglebib boxed boxed_publication">bibtex</a>

                  <p align="justify"> <i id="whirl_abs4" style="display: none;">Medical image classification is often hindered by limited labeled data, making few-shot learning (FSL) a crucial approach. However, conventional FSL methods struggle with the high variance present in medical images. Consequently, we propose Dual-Level Adaptive Sampling (DLAS) to enhance FSL by strategically selecting the most informative class combinations and instances within each class. During training, our class sampling policy prioritizes challenging, underconfident classes to foster continual learning. Subsequently, instances within each class are weighted according to their true class probability, ensuring the selection of hard-to-classify samples for improved generalization. This dual-level strategy is seamlessly integrated into prototypical networks within a metric-based meta-learning framework and can be adapted to various few-shot learning models. We validate our approach through extensive quantitative and qualitative analyses on three benchmark medical imaging datasets-Derm7pt, PathMNIST, and ChestMNIST. Results show that our sampling framework significantly improves model generalization and classification performance compared to existing State-of-the-art methods.</i></p>
              
              <pre xml:space="preserve" style="display: none;">
@INPROCEEDINGS{11167098,
  author={Chowdhury, Ranjana Roy and Niyaz, Usma and Bathula, Deepti R.},
  booktitle={2025 IEEE 7th Symposium on Computers & Informatics (ISCI)}, 
  title={Dual-Level Adaptive Sampling for Enhanced Few-Shot Medical Image Classification}, 
  year={2025},
  volume={},
  number={},
  pages={408-413},
  keywords={Metalearning;Training;Continuing education;Adaptation models;Refining;Prototypes;Few shot learning;Informatics;Biomedical imaging;Image classification;Meta Learning;Few Shot Learning;Prototypical Networks;Adaptive Sampling},
  doi={10.1109/ISCI65687.2025.11167098}}
    
    
              </pre>
                  </div>
                </td>
              </tr>


              <tr>
                <td width="20%" valign="top" align="center"><a href="images/miccai24-lsplus.png" target="_blank" title="Open image (HD)">
                <img src="images/miccai24-lsplus.png" alt="miccai2024" width="160" height="120" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="https://papers.miccai.org/miccai-2024/paper/3276_paper.pdf" target="_blank">
                    <papertitle>LS+: Informed Label Smoothing for Improving Calibration in Medical Image Classification</papertitle>
                  </a>
                  <br>
                  Abhishek Singh Sambyal, <strong> Usma Niyaz</strong>, Saksham Shrivastava, Narayanan C. Krishnan, Deepti R. Bathula
                  <br>
                  <em>27th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, 2024
              
                  <div class="paperlinks" id="trau5">
                  <a href="javascript:toggleblock('whirl_abs5')" class="boxed boxed_publication">abstract</a>
                  <a href="https://papers.miccai.org/miccai-2024/paper/3276_paper.pdf" class="boxed boxed_publication">journal</a>
                  <!-- <a href="https://www.dropbox.com/s/5adrl9tyx6s67ii/Poster_ISBI_2022.pdf?dl=0" class="boxed boxed_publication">poster</a> -->
                  <a shape="rect" href="javascript:togglebib('trau5')" class="togglebib boxed boxed_publication">bibtex</a>

                  <p align="justify"> <i id="whirl_abs5" style="display: none;">Deep Neural Networks (DNNs) exhibit exceptional performance in various tasks; however, their susceptibility to miscalibration poses challenges in healthcare applications, impacting reliability and trustworthiness. Label smoothing, which prefers soft targets based on uniform distribution over labels, is a widely used strategy to improve model calibration. We propose an improved strategy, Label Smoothing Plus (LS+), which uses class-specific prior that is estimated from validation set to account for current model calibration level. We evaluate the effectiveness of our approach by comparing it with state-of-the-art methods on three benchmark medical imaging datasets, using two different architectures and several performance and calibration metrics for the classification task. Experimental results show notable reduction in calibration error metrics with nominal improvement in performance compared to other approaches, suggesting that our proposed method provides more reliable prediction probabilities.</i></p>
              
              <pre xml:space="preserve" style="display: none;">
@InProceedings{lsplus,
  author="Sambyal, Abhishek Singh
  and Niyaz, Usma
  and Shrivastava, Saksham
  and Krishnan, Narayanan C.
  and Bathula, Deepti R.",
  editor="Linguraru, Marius George
  and Dou, Qi
  and Feragen, Aasa
  and Giannarou, Stamatia
  and Glocker, Ben
  and Lekadir, Karim
  and Schnabel, Julia A.",
  title="LS+: Informed Label Smoothing for Improving Calibration in Medical Image Classification",
  booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024",
  year="2024",
  pages="513--523",
  isbn="978-3-031-72117-5"
  }
    
    
              </pre>
                  </div>
                </td>
              </tr>

              <tr>
                <td width="20%" valign="top" align="center"><a href="images/isbi2024.png" target="_blank" title="Open image (HD)">
                <img src="images/isbi2024.png" alt="isbi2024" width="160" height="120" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="https://ieeexplore.ieee.org/document/10635879" target="_blank">
                    <papertitle>Wavelet-Based Feature Compression for Improved Knowledge Distillation</papertitle>
                  </a>
                  <br>
                  <strong>Usma Niyaz</strong>, Abhishek Singh Sambyal,  Deepti R. Bathula
                  <br>
                  <em>IEEE 21st International Symposium on Biomedical Imaging (ISBI)</em>, 2024
              
                  <div class="paperlinks" id="trau6">
                  <a href="javascript:toggleblock('whirl_abs6')" class="boxed boxed_publication">abstract</a>
                  <a href="https://ieeexplore.ieee.org/document/10635879" class="boxed boxed_publication">journal</a>
                  <!-- <a href="https://www.dropbox.com/s/5adrl9tyx6s67ii/Poster_ISBI_2022.pdf?dl=0" class="boxed boxed_publication">poster</a> -->
                  <a shape="rect" href="javascript:togglebib('trau6')" class="togglebib boxed boxed_publication">bibtex</a>

                  <p align="justify"> <i id="whirl_abs6" style="display: none;">Deep learning (DL) models can achieve state-of-the-art performance but at the cost of high computation and memory requirements. Due to their large capacity, DL models have the tendency to learn redundant features. In this work, we exploit this redundancy to improve model compression. Knowledge distillation (KD) aims to achieve model compression by transferring knowledge from a large, complex model to a simple, lightweight model. We propose an enhanced KD strategy that improves the efficiency of the distillation process by compressing the feature maps using Discrete Wavelet Transformation (DWT), which helps capture crucial features from complex biomedical signals. Retaining and sharing only the most informative and discriminating features facilitates more effective feature mimicking. Extensive experiments using two benchmark datasets for Melanoma and Histopathology image classification tasks demonstrate the superiority of our proposed method over existing techniques. We further establish the generalizability and robustness of our method using two different teacher-student architectures and ablation studies.</i></p>
              
              <pre xml:space="preserve" style="display: none;">
@INPROCEEDINGS{usma-waveletkd-isbi2024,
      author={Niyaz, Usma and Sambyal, Abhishek Singh and Bathula, Deepti R.},
      booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
      title={Wavelet-Based Feature Compression for Improved Knowledge Distillation}, 
      year={2024},
      pages={1-4},
      keywords={Knowledge engineering;Image coding;Histopathology;Computational modeling;Biological system modeling;Melanoma;Discrete wavelet transforms;Knowledge distillation;Wavelet;Teacher-student network;Online distillation;Denoising;Compression;Feature maps},
      doi={10.1109/ISBI56570.2024.10635879}
}
    
              </pre>
                  </div>
                </td>
              </tr>

              <tr>
                <td width="20%" valign="top" align="center"><a href="images/CBM2024.png" target="_blank" title="Open image (HD)">
                <img src="images/CBM2024.png" alt="cbm2024" width="160" height="120" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010482523012295" target="_blank">
                    <papertitle>Leveraging Different Learning Styles for Improved Knowledge Distillation in Biomedical Imaging</papertitle>
                  </a>
                  <br>
                  <strong>Usma Niyaz</strong>, Abhishek Singh Sambyal,  Deepti R. Bathula
                  <br>
                  <em>Computers in Biology and Medicine</em>, 2024
              
                  <div class="paperlinks" id="trau7">
                  <a href="javascript:toggleblock('whirl_abs7')" class="boxed boxed_publication">abstract</a>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010482523012295" class="boxed boxed_publication">journal</a>
                  <!-- <a href="https://www.dropbox.com/s/5adrl9tyx6s67ii/Poster_ISBI_2022.pdf?dl=0" class="boxed boxed_publication">poster</a> -->
                  <a shape="rect" href="javascript:togglebib('trau7')" class="togglebib boxed boxed_publication">bibtex</a>

                  <p align="justify"> <i id="whirl_abs7" style="display: none;">Learning style refers to a type of training mechanism adopted by an individual to gain new knowledge. As suggested by the VARK model, humans have different learning preferences, like Visual (V), Auditory (A), Read/Write (R), and Kinesthetic (K), for acquiring and effectively processing information. Our work endeavors to leverage this concept of knowledge diversification to improve the performance of model compression techniques like Knowledge Distillation (KD) and Mutual Learning (ML). Consequently, we use a single-teacher and two-student network in a unified framework that not only allows for the transfer of knowledge from teacher to students (KD) but also encourages collaborative learning between students (ML). Unlike the conventional approach, where the teacher shares the same knowledge in the form of predictions or feature representations with the student network, our proposed approach employs a more diversified strategy by training one student with predictions and the other with feature maps from the teacher. We further extend this knowledge diversification by facilitating the exchange of predictions and feature maps between the two student networks, enriching their learning experiences. We have conducted comprehensive experiments with three benchmark datasets for both classification and segmentation tasks using two different network architecture combinations. These experimental results demonstrate that knowledge diversification in a combined KD and ML framework outperforms conventional KD or ML techniques (with similar network configuration) that only use predictions with an average improvement of 2%. Furthermore, consistent improvement in performance across different tasks, with various network architectures, and over state-of-the-art techniques establishes the robustness and generalizability of the proposed model.</i></p>
              
              <pre xml:space="preserve" style="display: none;">
@article{NIYAZ-cbm-2023,
      title = {Leveraging different learning styles for improved knowledge distillation in biomedical imaging},
      journal = {Computers in Biology and Medicine},
      volume = {168},
      pages = {107764},
      year = {2024},
      issn = {0010-4825},
      doi = {https://doi.org/10.1016/j.compbiomed.2023.107764},
      author = {Usma Niyaz and Abhishek Singh Sambyal and Deepti R. Bathula},
}
    
              </pre>
                  </div>
                </td>
              </tr>

              <tr>
                <td width="20%" valign="top" align="center"><a href="images/cmpb-2023.png" target="_blank" title="Open image (HD)">
                <img src="images/cmpb-2023.png" alt="cmpb-2023" width="160" height="120" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0169260723004820" target="_blank">
                    <papertitle>Understanding Calibration of Deep Neural Networks for Medical Image Classification</papertitle>
                  </a>
                  <br>
                  Abhishek Singh Sambyal, <strong>Usma Niyaz</strong>, Deepti R. Bathula
                  <br>
                  <em>Computer Methods and Programs in Biomedicine</em>, 2023
              
                  <div class="paperlinks" id="trau8">
                  <a href="javascript:toggleblock('whirl_abs8')" class="boxed boxed_publication">abstract</a>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0169260723004820" class="boxed boxed_publication">journal</a>
                  <!-- <a href="https://www.dropbox.com/s/5adrl9tyx6s67ii/Poster_ISBI_2022.pdf?dl=0" class="boxed boxed_publication">poster</a> -->
                  <a shape="rect" href="javascript:togglebib('trau8')" class="togglebib boxed boxed_publication">bibtex</a>

                  <p align="justify"> <i id="whirl_abs8" style="display: none;">In the field of medical image analysis, achieving high accuracy is not enough; ensuring well-calibrated predictions is also crucial. Confidence scores of a deep neural network play a pivotal role in explainability by providing insights into the model's certainty, identifying cases that require attention, and establishing trust in its predictions. Consequently, the significance of a well-calibrated model becomes paramount in the medical imaging domain, where accurate and reliable predictions are of utmost importance. While there has been a significant effort towards training modern deep neural networks to achieve high accuracy on medical imaging tasks, model calibration and factors that affect it remain under-explored. To address this, we conducted a comprehensive empirical study that explores model performance and calibration under different training regimes. We considered fully supervised training, which is the prevailing approach in the community, as well as rotation-based self-supervised method with and without transfer learning, across various datasets and architecture sizes. Multiple calibration metrics were employed to gain a holistic understanding of model calibration. Our study reveals that factors such as weight distributions and the similarity of learned representations correlate with the calibration trends observed in the models. Notably, models trained using rotation-based self-supervised pretrained regime exhibit significantly better calibration while achieving comparable or even superior performance compared to fully supervised models across different medical imaging datasets. These findings shed light on the importance of model calibration in medical image analysis and highlight the benefits of incorporating self-supervised learning approach to improve both performance and calibration.</i></p>
              
              <pre xml:space="preserve" style="display: none;">
@article{understandingcalibration-2023,
      title = {Understanding calibration of deep neural networks for medical image classification},
      journal = {Computer Methods and Programs in Biomedicine},
      volume = {242},
      pages = {107816},
      year = {2023},
      issn = {0169-2607},
      doi = {https://doi.org/10.1016/j.cmpb.2023.107816},
      author = {Abhishek Singh Sambyal and Usma Niyaz and Narayanan C. Krishnan and Deepti R. Bathula},
  }
              </pre>
                  </div>
                </td>
              </tr>



              <!-- Publication Entries -->

              <tr>
                <td width="20%" valign="top" align="center"><a href="images/ISBI2022.png" target="_blank" title="Open image (HD)">
                <img src="images/ISBI2022.png" alt="isbi2022" width="160" height="120" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="https://ieeexplore.ieee.org/document/9761511" target="_blank">
                    <papertitle>Augmenting Knowledge Distillation with Peer-to-Peer Mutual Learning for Model Compression</papertitle>
                  </a>
                  <br>
                  <strong>Usma Niyaz</strong>, Deepti R. Bathula
                  <br>
                  <em>IEEE 19th International Symposium on Biomedical Imaging (ISBI)</em>, 2022
              
                    <div class="paperlinks" id="trau9">
                    <a href="javascript:toggleblock('whirl_abs9')" class="boxed boxed_publication">abstract</a>
                    <a href="https://ieeexplore.ieee.org/document/9761511" class="boxed boxed_publication">paper</a>
                    <!-- <a href="https://www.dropbox.com/s/5adrl9tyx6s67ii/Poster_ISBI_2022.pdf?dl=0" class="boxed boxed_publication">poster</a> -->
                    <a shape="rect" href="javascript:togglebib('trau9')" class="togglebib boxed boxed_publication">bibtex</a>

                    <p align="justify"> <i id="whirl_abs9" style="display: none;">Knowledge distillation (KD) is an effective model compression technique where a compact student network is taught to mimic the behavior of a complex and highly trained teacher network. In contrast, Mutual Learning (ML) provides an alternative strategy where multiple simple student networks benefit from sharing knowledge, even in the absence of a powerful but static teacher network. Motivated by these findings, we propose a single-teacher, multi-student framework that leverages both KD and ML to achieve better performance. Furthermore, an online distillation strategy is utilized to train the teacher and students simultaneously. To evaluate the performance of the proposed approach, extensive experiments were conducted using three different versions of teacher-student networks on benchmark biomedical classification (MSI vs. MSS) and object detection (Polyp Detection) tasks. Ensemble of student networks trained in the proposed manner achieved better results than the ensemble of students trained using KD or ML individually, establishing the benefit of augmenting knowledge transfer from teacher to students with peer-to-peer learning between students.</i></p>
              
                  <pre xml:space="preserve" style="display: none;">
@INPROCEEDINGS{9761511,
  author={Niyaz, Usma and Bathula, Deepti R.},
  booktitle={2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)}, 
  title={Augmenting Knowledge Distillation with Peer-to-Peer Mutual Learning for Model Compression}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  keywords={Knowledge engineering;Training;Biological system modeling;Object detection;Benchmark testing;Peer-to-peer computing;Task analysis;Knowledge distillation;Peer Mutual learning;Teacher-student network;Online distillation},
  doi={10.1109/ISBI52829.2022.9761511}
}
              </pre>
                  </div>
                </td>
              </tr>




              <tr>
                <td width="20%" valign="top" align="center"><a href="images/2018_advances.png" target="_blank" title="Open image (HD)">
                <img src="images/2018_advances.png" alt="adlt" width="160" height="120" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8745790" target="_blank">
                    <papertitle>Advances in Deep Learning Techniques for Medical Image Analysis</papertitle>
                  </a>
                  <br>
                  <strong> Usma Niyaz </strong>, Abhishek Singh Sambyal, Devanand
                  <br>
                  <em>Fifth International Conference on Parallel, Distributed and Grid Computing (PDGC)</em>, 2018
              
                  <div class="paperlinks" id="adlt">
                  <a href="javascript:toggleblock('adlt_abs')" class="boxed boxed_publication">abstract</a>
                  <a shape="rect" href="javascript:togglebib('adlt')" class="togglebib boxed boxed_publication">bibtex</a>
              
                  <p align="justify"> <i id="adlt_abs" style="display: none;">Deep learning is contributing to the high level of services to the healthcare sector. As the digital medical data is increasing exponentially with time, early detection and prediction of diseases are becoming more efficient because of the deep learning techniques which reduce the fatality rate to a great extent. The main focus of this paper is to provide the comprehensive review of deep learning in the domain of medical image processing and analysis. We have demonstrated the use of new deep learning architectures in oncology for the prediction of different types of cancer like the brain, lung, skin, etc. The state-of-the-art architectures effectively carry out analysis of 2D and 3D medical images to make the diagnosis of patients faster and more accurate. The use of popular approaches in machine learning such as ensemble and transfer learning with fine-tuning of parameters improve the performance of the deep neural networks in medical image analysis. The existing deep networks urge the new image classification network called Capsule Network (CapsNet) to make the classification and detection comparatively better. The equivariance characteristics of CapsNet make it more influential as it discourages the effect of any structural invariance of an input image on the network.</i></p>
              
              <pre xml:space="preserve" style="display: none;">
@inproceedings{adlt,
  title={Advances in Deep Learning Techniques for Medical Image 
  Analysis}, 
  author={Niyaz, Usma and Sambyal, Abhishek Singh and Devanand},
  booktitle={2018 Fifth International Conference on Parallel, 
  Distributed and Grid Computing (PDGC)}, 
  year={2018},
  doi={10.1109/PDGC.2018.8745790}}
              </pre>
                  </div>
                </td>
              </tr>




              <tr>
                <td width="20%" valign="top" align="center"><a href="images/2018_evaluation.png" target="_blank" title="Open image (HD)">
                <img src="images/2018_evaluation.png" alt="osm" width="160" height="120" style="padding:10px;border-radius:15px;border:1px solid black">
                </a></td>
                <td width="80%" valign="top">
                  <a href="https://link.springer.com/chapter/10.1007/978-981-15-0035-0_6" target="_blank">
                    <papertitle>Evaluation of Deep Learning Model with Optimizing and Satisficing Metrics for Lung
                      Segmentation</papertitle>
                  </a>
                  <br>
                  <strong> Usma Niyaz</strong>, Abhishek Singh Sambyal, Devanand
                  <br>
                  <em>Soft Computing for Problem Solving (SocProS)[Proceedings in AISC]</em>, 2018
              
                  <div class="paperlinks" id="osm">
                  <a href="javascript:toggleblock('osm_abs')" class="boxed boxed_publication">abstract</a>
                  <a shape="rect" href="javascript:togglebib('osm')" class="togglebib boxed boxed_publication">bibtex</a>
              
                  <p align="justify"> <i id="osm_abs" style="display: none;">The segmentation in medical image analysis is a crucial and prerequisite process during the diagnosis of the diseases. The need for segmentation is important to attain the region of interest where the probability of occurrence of an abnormality such as a nodule in the lungs or tumor in the brain is high. In this paper, we have proposed a new architecture called FS-Net which is a convolutional neural network- based model for the segmentation of lungs in CT scan images. It performs encoding of images into the feature maps and then decodes the feature maps into their respective lung masks. We have also trained the state-of-the-art U-Net on the same dataset and compared the results on the basis of optimizing and satisficing metrics. These metrics are useful for the selection of a better model with the maximum score at the satisfying condition. The FS-Net is computationally very efficient and achieves promising dice coefficient and loss score when compared with the U-Net taking one-third of the time.</i></p>
              
              <pre xml:space="preserve" style="display: none;">
@inproceedings{osm,
  title={Evaluation of Deep Learning Model with Optimizing 
  and Satisficing Metrics for Lung Segmentation},
  author={Niyaz, Usma and Singh Sambyal, Abhishek
  and Padha, Devanand},
  booktitle={Soft Computing for Problem Solving},
  year={2020}}
              </pre>
                  </div>
                </td>
              </tr>




              

            </tbody>
          </table>

          <!-- Teaching Assistant Section -->
          <!-- Teaching Assistant Section -->
          <!-- Teaching Assistant Section -->
          <!-- Teaching Assistant Section -->
          <!-- Teaching Assistant Section -->
          <!-- Teaching Assistant Section -->
          <table id="ta">
            <tbody>
              <tr>
                <td style="padding-top:10px;padding-left:20px;padding-bottom:20px;width:100%;vertical-align:middle">
                  <hr class="section-divider">
                  <heading>Teaching</heading>
                </td>
              </tr>
              <tr>
                <td>
                  <ul>
                    <li class="ta"> <strong>Artificial Neural Networks (CS504),</strong> 2024, 2025
                      <br>
                      Teaching Assistant with <a href="http://cse.iitrpr.ac.in/deepti/">Dr Deepti R Bathula</a>
                    </li>
                    <li class="ta"> <strong>Optimization for Machine Learning (CS560),</strong> 2024
                      <br>
                      Teaching Assistant with <a href="https://www.iitrpr.ac.in/saide/profile.faculty.php?mail=mudasir%40iitrpr.ac.in">Dr. Mudasir Ganaie</a>
                    </li>
                    <li class="ta"> <strong>Programming Paradigms & Pragmatics (CS202),</strong> 2021, 2022, 2023
                      <br>
                      Teaching Assistant with <a href="http://cse.iitrpr.ac.in/deepti/">Dr Deepti R Bathula</a>
                    </li>
                    <li class="ta"> <strong>Computer Graphics (CS515),</strong> 2023
                      <br>
                      Teaching Assistant with <a href="http://cse.iitrpr.ac.in/deepti/">Dr Deepti R Bathula</a>
                    </li>
                    <li class="ta"> <strong>Digital Image Processing & Analysis (CS517),</strong> 2022
                      <br>
                      Teaching Assistant with <a href="http://cse.iitrpr.ac.in/deepti/">Dr Deepti R Bathula</a>
                    </li>
                    <li class="ta"> <strong>Data Structures and Algorithms (CS506),</strong> 2021
                      <br>
                      Teaching Assistant with <a href="https://sites.google.com/view/goyalpuneet">Dr. Puneet Goyal</a>
                    </li>
                    
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table id="reviewer">
            <tbody>
              <tr>
                <td style="padding-top:10px;padding-left:20px;padding-bottom:20px;width:100%;vertical-align:middle">
                  <hr class="section-divider">
                  <heading>Reviewer</heading>
                </td>
              </tr>
              <tr>
                <td>
                  <ul>
                    <li class="ta"> <strong>IEEE International Symposium on Biomedical Imaging (ISBI),</strong> 
                    </li>
                    <li class="ta"> <strong>Medical Image Computing and Computer Assisted Intervention (MICCAI),</strong> 
                    </li>
                    <li class="ta"> <strong>Computers in Biology and Medicine,</strong> 
                    </li>
                    <li class="ta"> <strong>Knowledge Based Systems,</strong> 
                    </li>
                    <li class="ta"> <strong>Computers and Electrical Engineering,</strong> 
                    </li>
                    <li class="ta"> <strong>Computer Vision & Image Processing (CVIP),</strong> 
                    </li>
                    
                  </ul>
                </td>
            </tbody>
          </table>

          <table id="collaboration">
            <tbody>
              <tr>
                <td style="padding-top:10px;padding-left:20px;padding-bottom:20px;width:100%;vertical-align:middle">
                  <hr class="section-divider">
                  <heading>Collaboration</heading>
                </td>
              </tr>
              <tr>
                <td>
                  <ul>
                    <li class="ta"> <strong>Mom2b: Predicting peripartum depression using a smartphone application and digital phenotyping,</strong> Uppsala University, Sweden
                    </li>
                
                    
                  </ul>
                </td>
            </tbody>
          </table>

          <!-- Template Credit section -->
          <!-- Template Credit section -->
          <!-- Template Credit section -->
          <!-- Template Credit section -->
          <!-- Template Credit section -->
          <!-- Template Credit section -->
          <table 
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td>
                  <p align="right">
                    <font size="2.5">
                      Template from <a href="http://www.cs.berkeley.edu/~barron/">1</a>,<a
                        href="https://www.cs.cmu.edu/~dpathak/">2</a> and <a href="http://jeffdonahue.com/">3</a>
                    </font>
                <!-- <img src="images/halo-legendary-icon.png" width="32" height="32"> -->

                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
  
  <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td>
          <p align="right">
            <font size="1.5">
              Template from <a href="http://www.cs.berkeley.edu/~barron/">this</a>,<a
                href="https://www.cs.cmu.edu/~dpathak/">this</a> and <a href="http://jeffdonahue.com/">this</a>
            </font>
          </p>
        </td>
      </tr>
    </tbody>
  </table> -->

  <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
  <script>
    AOS.init();
  </script>
</body>

</html>
